{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def conv_features(text,fw, include_false=False) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "\n",
    "       Args:\n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word\n",
    "            in `text` must be in fw in order to be returned. This\n",
    "            prevents us from considering very rarely occurring words.\n",
    "\n",
    "       Returns:\n",
    "            A dictionary with the words in `text` that appear in `fw`.\n",
    "            Words are only counted once.\n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of\n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "\n",
    "    \"\"\"\n",
    "    ret_dict = dict()\n",
    "\n",
    "    present_tokens = set(text.split())\n",
    "\n",
    "    for token in feature_words:\n",
    "        if token in present_tokens:\n",
    "            ret_dict[token] = True\n",
    "        else:\n",
    "            if include_false: # include false\n",
    "                ret_dict[token] = False\n",
    "\n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# added libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- functions from past homework assignments for this course ---\n",
    "# Some punctuation variations\n",
    "punctuation = set(string.punctuation) # speeds up comparison\n",
    "# somehow, to_add[1] != to_add[2]\n",
    "to_add = ['`','’','’','•','›','«','×']\n",
    "punctuation.update(to_add)\n",
    "punctuation.remove('#')\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def contains_emoji(s):\n",
    "    emoji_count = emoji.emoji_count(s)\n",
    "    return(emoji_count > 0)\n",
    "\n",
    "def prepare(text, pipeline) :\n",
    "    '''\n",
    "        Chandler, John\n",
    "        August 22, 2022\n",
    "        ADS 509 Module 3: Group Comparison\n",
    "        Code Version: Git commit 0405f0f35f67edf62f95bba5052cc11efbda26c9\n",
    "        NLP Pipeline Transformer\n",
    "        https://github.com/37chandler/ads-tm-group-comp/blob/main/Group%20Comparison.ipynb\n",
    "    '''\n",
    "    tokens = str(text)\n",
    "    for transform in pipeline :\n",
    "        tokens = transform(tokens)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "def remove_punctuation(text, punct_set=punctuation):\n",
    "    '''\n",
    "        Chandler, John\n",
    "        August 22, 2022\n",
    "        ADS 509 Module 3: Group Comparison\n",
    "        Code Version: Git commit 0405f0f35f67edf62f95bba5052cc11efbda26c9\n",
    "        NLP Punctuation Remover\n",
    "        https://github.com/37chandler/ads-tm-group-comp/blob/main/Group%20Comparison.ipynb\n",
    "    '''\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def remove_stop(text) :\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in sw]\n",
    "    string_ =  ' '.join(tokens)\n",
    "    return(string_)\n",
    "\n",
    "def tokenize(text) :\n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That\n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "query = '''\n",
    "            SELECT text, party\n",
    "            FROM conventions\n",
    "        '''\n",
    "\n",
    "query_results = convention_cur.execute(query)\n",
    "\n",
    "text_prep_pipeline = [str.lower,\n",
    "                      remove_punctuation,\n",
    "                      remove_stop]\n",
    "\n",
    "for row in query_results :\n",
    "    text = prepare(text=row[0], pipeline=text_prep_pipeline)\n",
    "    party = row[1]\n",
    "    convention_data.append([text,party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['must come together defeat donald trump elect joe biden kamala harris next president vice president',\n",
       "  'Democratic'],\n",
       " ['joe always cared military families theyve much went iraq one generals said ” want share story you” daughters class christmas program playing ave maria one little girls burst tears teacher ran said “whats matter whats matter” said “thats song played daddys funeral died war” teacher idea little girls father fought war died night said staff im teacher better weve got better help military kids',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2383 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classifier 1 W/Out False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 6484.91it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = []\n",
    "for (text, party) in tqdm(convention_data):\n",
    "    featuresets.append((conv_features(text,feature_words), party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 7300.99it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in tqdm(convention_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n",
      "                    mike = True           Republ : Democr =     10.3 : 1.0\n",
      "                  prison = True           Republ : Democr =     10.3 : 1.0\n",
      "                    2018 = True           Republ : Democr =      9.9 : 1.0\n",
      "                   armed = True           Republ : Democr =      9.9 : 1.0\n",
      "                 culture = True           Republ : Democr =      9.9 : 1.0\n",
      "             regulations = True           Republ : Democr =      9.9 : 1.0\n",
      "                  relief = True           Republ : Democr =      9.9 : 1.0\n",
      "               wonderful = True           Republ : Democr =      9.9 : 1.0\n",
      "                  agenda = True           Republ : Democr =      9.7 : 1.0\n",
      "                    race = True           Republ : Democr =      9.7 : 1.0\n",
      "                  reform = True           Republ : Democr =      9.7 : 1.0\n",
      "                  cities = True           Republ : Democr =      9.5 : 1.0\n",
      "                       j = True           Republ : Democr =      9.2 : 1.0\n",
      "              appreciate = True           Republ : Democr =      9.0 : 1.0\n",
      "                   moved = True           Republ : Democr =      9.0 : 1.0\n",
      "                preserve = True           Republ : Democr =      9.0 : 1.0\n",
      "                  signed = True           Republ : Democr =      9.0 : 1.0\n",
      "                 freedom = True           Republ : Democr =      9.0 : 1.0\n",
      "            conservative = True           Republ : Democr =      8.8 : 1.0\n",
      "                   court = True           Republ : Democr =      8.8 : 1.0\n",
      "               defunding = True           Republ : Democr =      8.8 : 1.0\n",
      "             development = True           Republ : Democr =      8.8 : 1.0\n",
      "                 endless = True           Republ : Democr =      8.8 : 1.0\n",
      "                 fathers = True           Republ : Democr =      8.8 : 1.0\n",
      "               gentlemen = True           Republ : Democr =      8.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classifier 2 W/ False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 3357.68it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = []\n",
    "for (text, party) in tqdm(convention_data):\n",
    "    featuresets.append((conv_features(text,\n",
    "                                      feature_words,\n",
    "                                      include_false=True),\n",
    "                        party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 3186.81it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(conv_features(text,\n",
    "                              feature_words,\n",
    "                              include_false=True),\n",
    "                party) for (text, party) in tqdm(convention_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,\n",
    "                              feature_words,\n",
    "                              include_false=True),\n",
    "                party) for (text, party) in tqdm(convention_data)]\n",
    "\n",
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier_wFalse = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier_wFalse, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier_wFalse.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## My Observations\n",
    "Strangely enough, even though we include false values which increases test accuracy by roughly 20%, most informative features is the exact same for both classifiers.  Only difference is False values in our classifier 1 are None instead of False as shown in classifier 2.\n",
    "\n",
    "Another interesting find is that only 2 features, votes and climate, are the only two Democratic Party dominant words out of the top 50 informative features.  Thus, it appears that the classifier favors in recognizing if a text is Republican or not Republican."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 s, sys: 1.43 s, total: 5.73 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = '''\n",
    "           SELECT DISTINCT\n",
    "                  cd.candidate,\n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd\n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle\n",
    "               AND cd.candidate == tw.candidate\n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic')\n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        '''\n",
    "\n",
    "results = cong_db.execute(query)\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FROM https://stackoverflow.com/questions/24399820/expression-to-remove-url-links-from-twitter-tweet\n",
    "def html_remover(str_):\n",
    "    pattern = r'(http)\\S+'\n",
    "    text = re.sub(pattern, '', str_)\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 664656/664656 [00:25<00:00, 26509.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tweet_data = []\n",
    "\n",
    "for row in tqdm(results):\n",
    "    text_prep_pipeline = [str.lower,\n",
    "                          html_remover,\n",
    "                          remove_punctuation,\n",
    "                          remove_stop]\n",
    "\n",
    "    text = prepare(text=row[2].decode('utf-8'), pipeline=text_prep_pipeline)\n",
    "    party = row[1]\n",
    "\n",
    "    tweet_data.append([text, party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "tweet_data_sample = random.sample(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 10, we have 33151 as features in the model.\n",
      "CPU times: user 4.55 s, sys: 184 ms, total: 4.73 s\n",
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_cutoff = 10\n",
    "tokens = [w for t, p in tweet_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "\n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----------\n",
    "## Feature Building w/out false values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 664656/664656 [54:42<00:00, 202.51it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(conv_features(text,\n",
    "                              feature_words,\n",
    "                              include_false=False),\n",
    "                party) for (text, party) in tqdm(tweet_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "twitter_clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(twitter_clf, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample:\n",
    "\n",
    "    estimated_party = twitter_clf.classify(conv_features(tweet,\n",
    "                                                         feature_words,\n",
    "                                                         False))\n",
    "    print(f\"Clean tweet: {tweet}\\n\\n\")\n",
    "    print(f\"Actual party: {party}\\n\",\n",
    "          f'Classifier prediction {estimated_party}')\n",
    "    print(\"--\"*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp\n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = twitter_clf.classify(conv_features(tweet,\n",
    "                                                         feature_words,\n",
    "                                                         False))\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reflections\n",
    "\n",
    "Write a little about what you see in the results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It appears that our classifier favors classifying documents that are written by Democratics.  As we see with Democratic documents, the classifier was able to successfully classify 91.4% (=5174/(5174 + 487)) of all Democratic Documents.  While the accuracy performance of 61.53% (=2671/(2671 + 1670) clearly shows that the classifier struggled with Republican Documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Projekt X\n",
    "(project experiment)\n",
    "\n",
    "I tried training a clf with True & False values for our twitter data, but for some reason, my computer was highly inconsistent with building the feature set when I included false values utilizing list comprehension.  Thus, the internals the function \"conv_features\" was copied below to allow me to print certain things while training (such as print statements).  *Sidenote, sometimes the following list comprehension took ~6 seconds, other attempts lasted beyond 20 minutes, or worst of all... kill my kernel!  All attempts had no background software running other than OS essentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████▉                           | 101986/664656 [13:55<1:15:11, 124.73it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "featuresets = [(conv_features(text,\n",
    "                              feature_words,\n",
    "                              include_false=True),\n",
    "                party) for (text, party) in tqdm(tweet_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_size = 500\n",
    "test_set, train_set = feature_sets[:test_size], feature_sets[test_size:]\n",
    "twitter_clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(twitter_clf, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp\n",
    "\n",
    "    # get the estimated party\n",
    "    estimated_party = twitter_clf.classify(conv_features(tweet,\n",
    "                                                         feature_words,\n",
    "                                                         True))\n",
    "    results[party][estimated_party] += 1\n",
    "\n",
    "    if idx > num_to_score :\n",
    "        break\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Talk about overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
