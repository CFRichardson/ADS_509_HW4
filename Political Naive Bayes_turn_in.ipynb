{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Chris Richardson <br><br>\n",
    "\n",
    "Oct 3, 2022 <br><br>\n",
    "\n",
    "ADS-509-Fall <br><br>\n",
    "\n",
    "Github Link: [https://github.com/CFRichardson/USD_ADS_509_HW3](https://github.com/CFRichardson/ADS_509_HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def conv_features(text,fw, include_false=False) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "\n",
    "       Args:\n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word\n",
    "            in `text` must be in fw in order to be returned. This\n",
    "            prevents us from considering very rarely occurring words.\n",
    "\n",
    "       Returns:\n",
    "            A dictionary with the words in `text` that appear in `fw`.\n",
    "            Words are only counted once.\n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of\n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "\n",
    "    \"\"\"\n",
    "    ret_dict = dict()\n",
    "\n",
    "    present_tokens = set(text.split())\n",
    "\n",
    "    for token in feature_words:\n",
    "        if token in present_tokens:\n",
    "            ret_dict[token] = True\n",
    "        else:\n",
    "            if include_false: # include false\n",
    "                ret_dict[token] = False\n",
    "\n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# added libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- functions from past homework assignments for this course ---\n",
    "# Some punctuation variations\n",
    "punctuation = set(string.punctuation) # speeds up comparison\n",
    "# somehow, to_add[1] != to_add[2]\n",
    "to_add = ['`','’','’','•','›','«','×']\n",
    "punctuation.update(to_add)\n",
    "punctuation.remove('#')\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def contains_emoji(s):\n",
    "    emoji_count = emoji.emoji_count(s)\n",
    "    return(emoji_count > 0)\n",
    "\n",
    "def prepare(text, pipeline) :\n",
    "    '''\n",
    "        Chandler, John\n",
    "        August 22, 2022\n",
    "        ADS 509 Module 3: Group Comparison\n",
    "        Code Version: Git commit 0405f0f35f67edf62f95bba5052cc11efbda26c9\n",
    "        NLP Pipeline Transformer\n",
    "        https://github.com/37chandler/ads-tm-group-comp/blob/main/Group%20Comparison.ipynb\n",
    "    '''\n",
    "    tokens = str(text)\n",
    "    for transform in pipeline :\n",
    "        tokens = transform(tokens)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "def remove_punctuation(text, punct_set=punctuation):\n",
    "    '''\n",
    "        Chandler, John\n",
    "        August 22, 2022\n",
    "        ADS 509 Module 3: Group Comparison\n",
    "        Code Version: Git commit 0405f0f35f67edf62f95bba5052cc11efbda26c9\n",
    "        NLP Punctuation Remover\n",
    "        https://github.com/37chandler/ads-tm-group-comp/blob/main/Group%20Comparison.ipynb\n",
    "    '''\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def remove_stop(text) :\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in sw]\n",
    "    string_ =  ' '.join(tokens)\n",
    "    return(string_)\n",
    "\n",
    "def tokenize(text) :\n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That\n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "query = '''\n",
    "            SELECT text, party\n",
    "            FROM conventions\n",
    "        '''\n",
    "\n",
    "query_results = convention_cur.execute(query)\n",
    "\n",
    "text_prep_pipeline = [str.lower,\n",
    "                      remove_punctuation,\n",
    "                      remove_stop]\n",
    "\n",
    "for row in query_results :\n",
    "    text = prepare(text=row[0], pipeline=text_prep_pipeline)\n",
    "    party = row[1]\n",
    "    convention_data.append([text,party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['faced president cowardice joe biden man proven courage restore moral compass confronting challenges hiding undermining elections keep job',\n",
       "  'Democratic'],\n",
       " ['washington dc', 'Democratic']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2383 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classifier 1 W/Out False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 6732.32it/s]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in tqdm(convention_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classifier 2 W/ False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2541/2541 [00:00<00:00, 3272.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "feature_sets = [(conv_features(text,\n",
    "                              feature_words,\n",
    "                              include_false=True),\n",
    "                party) for (text, party) in tqdm(convention_data)]\n",
    "\n",
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_set, train_set = feature_sets[:test_size], feature_sets[test_size:]\n",
    "classifier_wFalse = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier_wFalse, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 radical = True           Republ : Democr =     35.1 : 1.0\n",
      "                   votes = True           Democr : Republ =     35.0 : 1.0\n",
      "             enforcement = True           Republ : Democr =     18.1 : 1.0\n",
      "                freedoms = True           Republ : Democr =     16.5 : 1.0\n",
      "                  signed = True           Republ : Democr =     16.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_wFalse.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## My Observations\n",
    "Strangely enough, even though we include false values which increases test accuracy by roughly 20%, most informative features is the exact same for both classifiers.  Only difference is False values in our classifier 1 are None instead of False as shown in classifier 2.\n",
    "\n",
    "Another interesting find is that only 2 features, votes and climate, are the only two Democratic Party dominant words out of the top 50 informative features.  Thus, it appears that the classifier favors in recognizing if a text is Republican or not Republican."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.61 s, sys: 1.69 s, total: 6.31 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = '''\n",
    "           SELECT DISTINCT\n",
    "                  cd.candidate,\n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd\n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle\n",
    "               AND cd.candidate == tw.candidate\n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic')\n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        '''\n",
    "\n",
    "results = cong_db.execute(query)\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Regex Pattern from by stackoverflow user \"zx81\" on\n",
    "# https://stackoverflow.com/questions/24399820/expression-to-remove-url-links-from-twitter-tweet\n",
    "\n",
    "\n",
    "def html_remover(str_):\n",
    "    pattern = r'(http)\\S+'\n",
    "    text = re.sub(pattern, '', str_)\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 664656/664656 [00:26<00:00, 25532.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tweet_data = []\n",
    "\n",
    "for row in tqdm(results):\n",
    "    text_prep_pipeline = [str.lower,\n",
    "                          html_remover,\n",
    "                          remove_punctuation,\n",
    "                          remove_stop]\n",
    "\n",
    "    text = prepare(text=row[2].decode('utf-8'), pipeline=text_prep_pipeline)\n",
    "    party = row[1]\n",
    "\n",
    "    tweet_data.append([text, party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "tweet_data_sample = random.sample(tweet_data,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean tweet: catch realerincruz show right\n",
      "\n",
      "\n",
      "Actual party: Republican\n",
      " Classifier prediction Republican\n",
      "--------------------\n",
      "Clean tweet: thank pennsylvania governor tom wolf register vote pennsylvania online\n",
      "\n",
      "\n",
      "Actual party: Democratic\n",
      " Classifier prediction Democratic\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample:\n",
    "\n",
    "    estimated_party = classifier_wFalse.classify(conv_features(tweet,\n",
    "                                                         feature_words,\n",
    "                                                         False))\n",
    "    print(f\"Clean tweet: {tweet}\\n\\n\")\n",
    "    print(f\"Actual party: {party}\\n\",\n",
    "          f'Classifier prediction {estimated_party}')\n",
    "    print(\"--\"*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predictions w/ False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 57, 'Democratic': 4180}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 45, 'Democratic': 5720})})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp\n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier_wFalse.classify(conv_features(tweet,\n",
    "                                                         feature_words,\n",
    "                                                         True))\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predictions w/out False Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3785, 'Democratic': 557}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4897, 'Democratic': 763})})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary of counts by actual party and estimated party.\n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp\n",
    "\n",
    "    # get the estimated party\n",
    "    estimated_party = classifier_wFalse.classify(conv_features(tweet,\n",
    "                                                               feature_words,\n",
    "                                                               False))\n",
    "\n",
    "    results[party][estimated_party] += 1\n",
    "\n",
    "    if idx > num_to_score :\n",
    "        break\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reflections\n",
    "\n",
    "Write a little about what you see in the results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It appears that our classifier favors classifying documents that are written by Democratics.  As we see with Democratic documents, the classifier without False Values overfitted and practically thought every document is from the Democratic Party; with a recall of 1.35% (=57/(57 + 4180)) when attempting to classify a Republican Document and 99.22% recall score  when classifying a Democratic Document.\n",
    "\n",
    "While the classifier without False values overfitted again, but this time the classifier practically always classifying a document as Republican with an recall of 88.17% (=3785/(3785 + 557)) for Republican Documents but a measly recall of 13.48% when classifying for Democratic Documents.  Clearly the utilization of sparse information leads to overfitting, most likely due to the curse of dimensionality (too many features).\n",
    "\n",
    "Recall = TP/(TP + FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References\n",
    "--------\n",
    "* zx81 (2014, June 25). Expression to remove URL links from Twitter tweet. StackOverFlow. https://stackoverflow.com/questions/24399820/expression-to-remove-url-links-from-twitter-tweet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
